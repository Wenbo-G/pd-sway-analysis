{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b72bbf92",
   "metadata": {},
   "source": [
    "This is the jupyter notebook for the paper called\n",
    "# Which features of postural sway are effective in distinguishing Parkinsonâ€™s disease patients from controls? An experiment\n",
    "\n",
    "In here we use postural sway data from the Canberra Hospital. We featurise this using the features from a recent systematic review: https://onlinelibrary.wiley.com/doi/10.1002/brb3.1929.\n",
    "\n",
    "The postural sway data is first preprocessed. The features are then extracted, for which the effectiveness is calculated and compared, according to the claims made in the literature review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5a32e3",
   "metadata": {},
   "source": [
    "## Load the data and clean it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36863b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "pio.renderers.default = 'notebook'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cab0f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from package import featureHandler as fh\n",
    "from package import dataHandler as dh\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_rows', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e98f909d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 24 PD patients, 16 males and 8 females, with age 67.00 (+- 9.06), UPDRS 25.50 (+- 13.08), and yrs since diagnosis 6.98 (+- 5.20)\n",
      "\tThe 16 males have age 69.81 (+- 8.91), UPDRS 26.34 (+- 13.22), and yrs since diagnosis 6.92 (+- 4.73)\n",
      "\tThe 8 females have age 60.57 (+- 5.74), UPDRS 23.57 (+- 13.58), and yrs since diagnosis 7.11 (+- 6.49)\n",
      "There are 15 control participants, 5 males and 10 females, with age 72.40 (+- 10.56)\n",
      "\tThe 5 males have age 67.60 (+- 9.18)\n",
      "\tThe 10 females have age 72.40 (+- 10.56)\n"
     ]
    }
   ],
   "source": [
    "#Get list of participant as well as their demographics\n",
    "participants = pd.read_csv(join('Data','participants.csv'))\n",
    "participant_information = pd.read_csv(join('Data','participant information.csv'))\n",
    "participants = dh.df_retrieve(participants,{'Greenlight':1})\n",
    "participants = participants.merge(participant_information,on='Participant',how='left')\n",
    "is_PD = ['P' in p for p in participants['Participant']]\n",
    "participants['is PD'] = is_PD\n",
    "\n",
    "#Get the list of postural sway files I have available\n",
    "path = join('Data','Postural Sway')\n",
    "swayfiles = [f for f in listdir(path) if isfile(join(path, f)) and ('.mat' in f)]\n",
    "\n",
    "#Join the list of participants with their corresponding postural sway files\n",
    "participants.index = participants['Participant']\n",
    "for swayfile in swayfiles:\n",
    "    if swayfile[:4] in participants['Participant'].tolist(): #assumes swayfiles are named PXXX_EY.mat\n",
    "        if 'eo' in swayfile.lower():\n",
    "            participants.at[swayfile[:4],'EO sway file'] = swayfile\n",
    "        elif 'ec' in swayfile.lower():\n",
    "            participants.at[swayfile[:4],'EC sway file'] = swayfile\n",
    "        else:\n",
    "            print('What visual state is this?? The name is %s' % swayfile)\n",
    "participants = participants[['Age','Sex','Yrs since diagnosis','UPDRS','is PD','EC sway file','EO sway file']]\n",
    "\n",
    "#Print data summary\n",
    "dh.data_summary(participants)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917f11e5",
   "metadata": {},
   "source": [
    "## Preprocess the data\n",
    "This consists of cutting all data to be consistently 90 seconds long, scaling from metres to millimetres, lowpass filtered at 20 Hz, cutting out the first and last 15 seconds, demeaning the data, and then finally decimating the data to the desired sampling frequency.\n",
    "\n",
    "Decimation occurs with an antialiasing filter at $f_s/2$ Hz with an 8th order Chebyshev type 1 filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6259a7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(data, lowpass_cutoff, lowpass_order, decimate_order, demean=False, scale=1):\n",
    "    \"\"\"Preprocess the data. This assumes the data hasnt been played with yet, i.e., fs = 1000\n",
    "    A scale=1 means the units are in meters\"\"\"\n",
    "    \n",
    "    #If the data is 120 seconds, cut to 90 by removing last 30 seconds\n",
    "    data = [d[:90000] for d in data]\n",
    "    data[4] = data[4] * scale\n",
    "    data[5] = data[5] * scale\n",
    "    \n",
    "    #Low pass filter\n",
    "    sos = signal.butter(lowpass_order, lowpass_cutoff, 'lowpass', fs=1000, output='sos')\n",
    "    data = [data[0]] + [signal.sosfilt(sos,d) for d in data[1:]]\n",
    "    \n",
    "    #Remove first and last 15 seconds, resulting in a 60 second sample\n",
    "    #This also removes artifacts in the butterworth filter, as it assumes a start from 0, 0, which is not the case\n",
    "    data = [d[15000:75000] for d in data]\n",
    "    \n",
    "    #Recenters the data if needed, but not t and Fz\n",
    "    if demean: data = [data[0], data[1]] + [d - d.mean() for d in data[2:]]\n",
    "    \n",
    "    #Adjust time so that it starts from 0\n",
    "    data[0] = data[0] - min(data[0])\n",
    "    \n",
    "    #Decimate\n",
    "    data = dh.decimate(data,decimate_order)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882d2ad2",
   "metadata": {},
   "source": [
    "## Extract features\n",
    "\n",
    "The features are saved as a dictionary with the keys as ['10 Hz', '20 Hz', '40 Hz', '100 Hz']. The corresponding values are a DataFrame with columns as features and rows as participants. Note that EO and EC features from one participant is under the same row as different features (columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "380e4b7e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###############################################\n",
    "overwrite = False\n",
    "###############################################\n",
    "\n",
    "data_path = join('Data','Postural Sway')\n",
    "\n",
    "#Creating a base results DataFrame to copy when using different sampling rates\n",
    "base_feature_names,visual_feature_names = fh.get_feature_names()\n",
    "feature_names = list(itertools.chain(*[[name+'_EC',name+'_EO'] for name in base_feature_names])) + visual_feature_names\n",
    "base_features = pd.DataFrame(columns = feature_names)\n",
    "\n",
    "#Creating copies for the given sampling rates\n",
    "decimate_orders = [[10,10], [10,5], [5,5] ,10] #10, 20, 40, 100 Hz\n",
    "features = {}\n",
    "\n",
    "if not overwrite:\n",
    "    if isfile(join('Results','features.pickle')):\n",
    "        features = dh.pickleLoad(path=join('Results','features.pickle'))\n",
    "    else:\n",
    "        print('features.pickle doesnt exist')\n",
    "else:\n",
    "    if isfile(join('Results','features.pickle')): print('features.pickle already exists. We are now overwriting this')\n",
    "    for decimate_order in decimate_orders:\n",
    "        fs = 1000/np.prod(decimate_order)\n",
    "        features['%i Hz' % fs] = copy.copy(base_features)\n",
    "        lowpass_cutoff = 20\n",
    "        for participant in participants.index:\n",
    "            eo_sway_file = participants.loc[participant,'EO sway file']\n",
    "            ec_sway_file = participants.loc[participant,'EC sway file']\n",
    "\n",
    "            if isinstance(eo_sway_file,str): #if the path is a string, i.e., not NaN\n",
    "                eo_data = dh.load_sway_file(join(data_path,eo_sway_file))\n",
    "                eo_data = process(eo_data,lowpass_cutoff=lowpass_cutoff,lowpass_order=4,decimate_order=decimate_order,demean=True,scale=1000)\n",
    "                eo_features = fh.get_all_features(eo_data)\n",
    "                eo_features = {key+'_EO':val for key,val in eo_features.items()}\n",
    "\n",
    "            if isinstance(ec_sway_file,str): #if the path is a string, i.e., not NaN\n",
    "                ec_data = dh.load_sway_file(join(data_path,ec_sway_file))\n",
    "                ec_data = process(ec_data,lowpass_cutoff=lowpass_cutoff,lowpass_order=4,decimate_order=decimate_order,demean=True,scale=1000)\n",
    "                ec_features = fh.get_all_features(ec_data)\n",
    "                ec_features = {key+'_EC':val for key,val in ec_features.items()}\n",
    "\n",
    "            if isinstance(eo_sway_file,str) and isinstance(ec_sway_file,str):\n",
    "                eoec_features = fh.get_all_visual_features(eo_data, ec_data)\n",
    "\n",
    "            features['%i Hz' % fs].loc[participant] = {**eo_features, **ec_features, **eoec_features} #merge dict\n",
    "\n",
    "    #save these results\n",
    "#    dh.pickleSave(path=join('Results','features.pickle'),obj=features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bc61a4",
   "metadata": {},
   "source": [
    "## Removing perfectly correlated features\n",
    "We have identified the features that are perfectly correlated in the notebook: \"Perfectly correlated features\"\n",
    "\n",
    "The pairs are:\n",
    "- 'area90_ML', 'rms_displacement_ML'\n",
    "- 'area90_ML', 'std_displacement_ML'\n",
    "- 'area90_AP', 'rms_displacement_AP'\n",
    "- 'area90_AP', 'std_displacement_AP'\n",
    "- 'pathlength', 'avg_velocity'\n",
    "- 'pathlength', 'swayvector_length'\n",
    "- 'pathlength_ML', 'avg_velocity_ML'\n",
    "- 'pathlength_AP', 'avg_velocity_AP'\n",
    "- 'rms_displacement', 'planardeviation'\n",
    "- 'rms_displacement_ML', 'std_displacement_ML'\n",
    "- 'rms_displacement_AP', 'std_displacement_AP'\n",
    "- 'avg_displacement', 'avg_radius'\n",
    "- 'avg_velocity', 'swayvector_length'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f2faf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Theres quite a few. To get rid of these perfect correlations, we remove: \n",
    "features_to_remove = ['std_displacement_ML',\n",
    "                      'std_displacement_AP',\n",
    "                      'planardeviation',\n",
    "                      'avg_velocity_AP',\n",
    "                      'avg_velocity_ML',\n",
    "                      'area90_AP',\n",
    "                      'area90_ML',\n",
    "                      'swayvector_length',\n",
    "                      'avg_radius',\n",
    "                      'avg_velocity']\n",
    "\n",
    "#Remove it from base_feature_names\n",
    "for name in features_to_remove: base_feature_names.remove(name)\n",
    "    \n",
    "features_to_remove = [f+'_EO' for f in features_to_remove] + [f+'_EC' for f in features_to_remove] #include back EO/EC\n",
    "\n",
    "for fs_key in features.keys():\n",
    "    ind = features[fs_key].columns.isin(features_to_remove)\n",
    "    features[fs_key] = features[fs_key].loc[:,~ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de37675",
   "metadata": {},
   "source": [
    "## Hypothesis: Feature effectiveness with lower $f_s$ are also lower \n",
    "\n",
    "This was one of the claims stated in the literature review.\n",
    "\n",
    "To test this, we first determine the effectiveness of each feature. This was quantified in two ways:\n",
    "\n",
    "The first way was to simply calcualted the effect size of each feature. The effect size we use here is Hedges' $g$, as it handles large differences in variances and number of samples better than other effect sizes. This is defined as \n",
    "\\begin{equation}\n",
    "    \\frac{\\overline{x}_1 - \\overline{x}_2}{s^*}, \\quad \\textrm{where} \\quad s^* = \\sqrt{\\frac{(n_1-1)s^2_1 + (n_2-1)s^2_2}{n_1+n_2-2}}\n",
    "\\end{equation}\n",
    "\n",
    "The second way was to use machine learning. Here we perform out-of-sample testing with 10-times repeated 5-fold cross validation using a classification model. The data was randomly paired across PD and control classes by matching the sex and (roughly) the age of participants. This mean that the 39 participants were reduced to 30 for machine learning evaluations. The model we use here is an soft ensemble of an rbf-SVM, logistic regression with L2 penalty, and random forests with 100 trees. This evaluation was repeated for every single feature, resulting in an averaged AUROC which quantifies effectiveness of that feature.\n",
    "\n",
    "The effectiveness (both ES and AUROC) of features across sampling frequencies are compared by scaling them by the maximum value across sampling frequencies, so that the effectiveness of each feature is represented as a percentage of the maximum value. This distribution of percentages for a given sampling frequency is compared to the ideal (1 with no divergence) using the Wasserstein metric, and visually compared with a kernel density estimate.\n",
    "\n",
    "\n",
    "Additionally, we perform the same machine learning experiment but with multiple features. Here we use all features at once, and we use several feature selection methods. These were a wrapper method with a linear-SVM with L1 penalty, a minimum redundance method by choosing features with least correlation to each other, a maximum relevance method by choosing features most relevant to the target variable (defined with mutual information and ANOVA F-value). We also used PCA, though this isn't really a feature selection method, it reduces dimensionality and serves a similar function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cc4bbe",
   "metadata": {},
   "source": [
    "### Now looking at the effectiveness of using a subset of all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2dc432b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from plotly import graph_objects as go\n",
    "from package import resultsHandler as rh\n",
    "from package import ml\n",
    "import plotly.figure_factory as ff\n",
    "import baycomp\n",
    "from package import effect_sizes as es\n",
    "from scipy.stats import wasserstein_distance\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "dpi=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c11fd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, SelectFromModel, mutual_info_classif, f_classif\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "55974087",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUROCs = dh.pickleLoad(join('Results','AUROCs.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5189e28d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def keys():\n",
    "    return ['10 Hz','20 Hz','40 Hz','100 Hz']\n",
    "\n",
    "feature_selection_results = pd.DataFrame()\n",
    "feature_selection_full_results = pd.DataFrame(columns = keys(),dtype=object)\n",
    "##################################################################\n",
    "repetitions=10\n",
    "seeds=list(range(repetitions))\n",
    "return_dataframe=True\n",
    "SelectKBest_settings = list(itertools.product((f_classif, mutual_info_classif),list(range(2,24,2))))\n",
    "lsvc_C_settings = (0.1, 1.0, 10.0, 0.08, 0.8, 8.0, 0.3, 3.0)\n",
    "#PCA_settings = list(range(2,24))\n",
    "mincorr_settings = list(range(24))\n",
    "##################################################################\n",
    "        \n",
    "    \n",
    "for func, k in SelectKBest_settings:\n",
    "    \n",
    "    for fs_key in keys():\n",
    "        feature_selector = SelectKBest(func, k=k)\n",
    "\n",
    "        AUROC = ml.get_all_AUROCs_with_feature_selection(participants,features[fs_key],feature_selector,repetitions,return_dataframe,seeds)\n",
    "        feature_selection_full_results.at['%s, features=%i, AUROC' % (func.__name__,k), fs_key] = AUROC.T.unstack().values\n",
    "        AUROC = AUROC.mean(axis=1)\n",
    "        \n",
    "        feature_selection_results.at['%s, features=%i, AUROC mean' % (func.__name__,k), fs_key] = AUROC.mean()\n",
    "        feature_selection_results.at['%s, features=%i, AUROC std' % (func.__name__,k), fs_key] = AUROC.std()       \n",
    "        #print(fs_key,k,func)\n",
    "\n",
    "#################################\n",
    "\n",
    "for C in lsvc_C_settings:    \n",
    "    \n",
    "    for fs_key in keys():        \n",
    "        model = LinearSVC(C=C, penalty=\"l1\", dual=False, max_iter=100000, random_state=0) \n",
    "        feature_selector = SelectFromModel(model, prefit=False)\n",
    "        \n",
    "        AUROC = ml.get_all_AUROCs_with_feature_selection(participants,features[fs_key],feature_selector,repetitions,return_dataframe,seeds)\n",
    "        feature_selection_full_results.at['LinearSVC, C=%.3f, AUROC' % (C), fs_key] = AUROC.T.unstack().values\n",
    "        AUROC = AUROC.mean(axis=1)\n",
    "\n",
    "        feature_selection_results.at['LinearSVC, C=%.3f, AUROC mean' % (C), fs_key] = AUROC.mean()\n",
    "        feature_selection_results.at['LinearSVC, C=%.3f, AUROC std' % (C), fs_key] = AUROC.std()       \n",
    "        #print(fs_key,C)\n",
    "        \n",
    "#################################\n",
    "\n",
    "#for n_components in PCA_settings:\n",
    "    \n",
    "#    for fs_key in features.keys():        \n",
    "#        feature_selector = PCA(n_components=n_components)\n",
    "        \n",
    "#        AUROC = ml.get_all_AUROCs_with_feature_selection(participants,features[fs_key],feature_selector,repetitions,return_dataframe,seeds)\n",
    "#        feature_selection_full_results.at['n_components=%i, AUROC' % (n_components), fs_key] = AUROC.T.unstack().values\n",
    "#        AUROC = AUROC.mean(axis=1)\n",
    "\n",
    "#        feature_selection_results.at['n_components=%i, AUROC mean' % (n_components), fs_key] = AUROC.mean()\n",
    "#        feature_selection_results.at['n_components=%i, AUROC std' % (n_components), fs_key] = AUROC.std()\n",
    "\n",
    "###################################\n",
    "\n",
    "for fs_key in keys():\n",
    "    corr=features[fs_key].corr()\n",
    "    best_feature = AUROCs[fs_key].mean(axis=1).idxmax()\n",
    "    all_features = [best_feature]\n",
    "    \n",
    "    for i in mincorr_settings:\n",
    "        c = abs(corr[all_features]).sum(axis=1)\n",
    "        all_features.append(c.idxmin())\n",
    "\n",
    "        AUROC = ml.get_all_AUROCs_with_feature_selection(participants,features[fs_key][all_features],None,repetitions,True,seeds)\n",
    "        feature_selection_full_results.at['mincorr num_features=%i, AUROC'%len(all_features), fs_key] = AUROC.T.unstack().values\n",
    "        AUROC = AUROC.mean(axis=1)\n",
    "        \n",
    "        feature_selection_results.at['mincorr num_features=%i, AUROC mean'%len(all_features),fs_key] = AUROC.mean()\n",
    "        feature_selection_results.at['mincorr num_features=%i, AUROC std'%len(all_features),fs_key] = AUROC.std()\n",
    "        #print(fs_key,i)\n",
    "\n",
    "###################################\n",
    "\n",
    "print('Overall feature selection results')\n",
    "feature_selection_results_summary = pd.DataFrame()\n",
    "idxmax = feature_selection_results.idxmax()\n",
    "kf_results_selected = {}\n",
    "for fs_key in idxmax.index:\n",
    "    best_method_name = idxmax[fs_key]\n",
    "    feature_selection_results_summary.at['Best selection method',fs_key] = best_method_name[:-5]\n",
    "    feature_selection_results_summary.at['Mean of AUROC using best selection',fs_key] = feature_selection_results.loc[best_method_name,fs_key]\n",
    "    feature_selection_results_summary.at['Stdev of AUROC using best selection',fs_key] = feature_selection_results.loc[best_method_name[:-4] + 'std',fs_key]\n",
    "    kf_results_selected[fs_key] = feature_selection_full_results.loc[best_method_name[:-5],fs_key]\n",
    "display(feature_selection_results_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582092ef",
   "metadata": {},
   "source": [
    "Best feature selection methods:\n",
    "- 10 Hz: mincorr num_features=20\n",
    "- 20 Hz: mincorr num_features=2\n",
    "- 40 Hz: f_classif, features=22\n",
    "- 100 Hz: f_classif, features=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7ab43bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_key = '10 Hz'\n",
    "\n",
    "corr = features[fs_key].corr()\n",
    "best_feature = AUROCs[fs_key].mean(axis=1).idxmax()\n",
    "all_features = [best_feature]\n",
    "\n",
    "for i in range(19):\n",
    "    c = abs(corr[all_features]).sum(axis=1)\n",
    "    all_features.append(c.idxmin())\n",
    "best_subset_10 = all_features\n",
    "\n",
    "#########################\n",
    "\n",
    "fs_key = '20 Hz'\n",
    "\n",
    "corr = features[fs_key].corr()\n",
    "best_feature = AUROCs[fs_key].mean(axis=1).idxmax()\n",
    "all_features = [best_feature]\n",
    "\n",
    "c = abs(corr[all_features]).sum(axis=1)\n",
    "all_features.append(c.idxmin())\n",
    "best_subset_20 = all_features    \n",
    "    \n",
    "#########################\n",
    "\n",
    "fs_key = '40 Hz'\n",
    "\n",
    "func = f_classif\n",
    "k=22\n",
    "feature_selector = SelectKBest(func, k=k)\n",
    "_ = ml.get_all_AUROCs_with_feature_selection(participants,features[fs_key],feature_selector,repetitions,return_dataframe,seeds)\n",
    "best_subset_40 = features[fs_key].columns[feature_selector.get_support()]\n",
    "\n",
    "#########################\n",
    "\n",
    "fs_key = '100 Hz'\n",
    "\n",
    "func = f_classif\n",
    "k=20\n",
    "feature_selector = SelectKBest(func, k=k)\n",
    "_ = ml.get_all_AUROCs_with_feature_selection(participants,features[fs_key],feature_selector,repetitions,return_dataframe,seeds)\n",
    "best_subset_100 = features[fs_key].columns[feature_selector.get_support()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "07aae1d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak_displacement_backward_EC\n",
      "DTYC_EC\n",
      "DYL_EO\n",
      "%recurrence_ML_EC\n",
      "swaymovement_ML_EO\n",
      "bandpower_4-7_ML_EC\n",
      "RQA_trend_ML_EO\n",
      "bandpower_2-4_ML_EO\n",
      "%recurrence_ML_EO\n",
      "VRI_avg_displacement_AP\n",
      "%recurrence_AP_EO\n",
      "area95_majoraxis_angle_EO\n",
      "DTYC_EO\n",
      "VRI_swayarea\n",
      "%recurrence_AP_EC\n",
      "bandpower_4-7_ML_EC\n",
      "%recurrence_ML_EC\n",
      "avg_displacement_ML_EC\n",
      "VRI_avg_displacement_AP\n",
      "%recurrence_ML_EO\n",
      "\n",
      "\n",
      "area95_majoraxis_length_EC\n",
      "%recurrence_ML_EC\n",
      "\n",
      "\n",
      "area95_majoraxis_length_EC\n",
      "pathlength_ML_EC\n",
      "rms_displacement_EC\n",
      "rms_displacement_AP_EC\n",
      "displacement_range_AP_EC\n",
      "peak_displacement_AP_EC\n",
      "peak_displacement_forward_EC\n",
      "peak_displacement_backward_EC\n",
      "direction_index_ML_EC\n",
      "direction_index_AP_EC\n",
      "frequency95_AP_EO\n",
      "frequency90_AP_EO\n",
      "totalenergy_AP_EC\n",
      "DTXC_EO\n",
      "DTRC_EC\n",
      "DTRC_EO\n",
      "Y2_EC\n",
      "DRS_EC\n",
      "DYL_EC\n",
      "fractaldimension_EC\n",
      "swayvector_angle_EC\n",
      "RQA_trend_ML_EC\n",
      "\n",
      "\n",
      "pathlength_ML_EC\n",
      "rms_displacement_AP_EC\n",
      "displacement_range_AP_EC\n",
      "peak_displacement_AP_EC\n",
      "peak_displacement_forward_EC\n",
      "peak_displacement_backward_EC\n",
      "direction_index_ML_EC\n",
      "direction_index_AP_EC\n",
      "frequency95_AP_EO\n",
      "frequency90_AP_EO\n",
      "totalenergy_AP_EC\n",
      "DTXC_EO\n",
      "DTRC_EC\n",
      "DTRC_EO\n",
      "Y2_EC\n",
      "DRS_EC\n",
      "DYL_EC\n",
      "fractaldimension_EC\n",
      "swayvector_angle_EC\n",
      "RQA_trend_ML_EC\n"
     ]
    }
   ],
   "source": [
    "for p in best_subset_10:\n",
    "    print(p)\n",
    "    \n",
    "print('\\n')\n",
    "for p in best_subset_20:\n",
    "    print(p)\n",
    "    \n",
    "print('\\n')\n",
    "for p in best_subset_40:\n",
    "    print(p)\n",
    "    \n",
    "print('\\n')\n",
    "for p in best_subset_100:\n",
    "    print(p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (PD Sway)",
   "language": "python",
   "name": "pd_sway"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
